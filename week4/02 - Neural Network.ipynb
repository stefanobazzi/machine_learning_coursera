{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Reresentation\n",
    "\n",
    "Neouron = recieve some values a input adn give one value as output\n",
    "\n",
    "#### Neuron model: Logistic unit\n",
    "Some inputs e.g $ x1, x2, x3$ and gives one output $h_\\theta(x) = \\frac{1}{1 +e^{-\\theta^Tx}}$\n",
    "\n",
    "Sigmoid = activation function   \n",
    "weights = theta vector(parameters)   \n",
    "\n",
    "layer 1 = input layer, bias term neuron optional  \n",
    "layer 2 ... = hidden layer   \n",
    "layer n = output layer    \n",
    "\n",
    "$a_i^{(j)} = $ activation of unit i in layer j  \n",
    "$\\Theta^{(j)}$ = matrix of weights controlling function mapping from layer j to layer j+1\n",
    "\n",
    "e.g one hidden layer\n",
    "$\\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\rightarrow \\begin{bmatrix} a_1{(2)} \\\\ a_2{(2)} \\\\  a_3{(3)} \\end{bmatrix}  \\rightarrow h_\\Theta(x)$\n",
    "\n",
    "$a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3) $    \n",
    "$a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3) $     \n",
    "$a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3) $   \n",
    "\n",
    "$h_\\Theta(x) = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)} + \\Theta_{13}^{(2)}a_3^{(2)}) $\n",
    "\n",
    "If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\\Theta_j$ will be of dimension $s_{j+1}Ã—($_j+1)$.\n",
    "\n",
    "The +1 comes from the addition in $\\Theta^{(j)}$ of the \"bias nodes,\" $x_0$ and $\\Theta_0^{(j)}$. In other words the output nodes will not include the bias nodes while the inputs will."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Representation II\n",
    "\n",
    "#### Foward implementation: Vectorized implemetation\n",
    "Consider this implemetation:\n",
    "\n",
    "\n",
    "$a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3) $    \n",
    "$a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3) $     \n",
    "$a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3) $   \n",
    "\n",
    "$h_\\Theta(x) = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)} + \\Theta_{13}^{(2)}a_3^{(2)}) $\n",
    "\n",
    "$a^{(12)}$\n",
    "\n",
    "$x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$ \n",
    "\n",
    "$z^{(2)} = \\begin{bmatrix} z_1^{(2)} \\\\ z_2^{(2)} \\\\  z_3^{(3)} \\end{bmatrix}$\n",
    "\n",
    "$a^{(1)} = x$    \n",
    "$z^{(2)} = \\Theta^{(1)}x = \\Theta^{(1)}a^{(1)}$    \n",
    "$a^{(2)} = g(z^{(2)})$     \n",
    "add $a_0^{(2)} = 1$    \n",
    "$z^{(3)} = \\Theta^{(2)}a^{(2)}$     \n",
    "$h_\\Theta(x) = a^{(3)} = g(z^{(3)})$   \n",
    "\n",
    "Notice that in this last step, between layer j and layer j+1, we are doing exactly the same thing as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypotheses.\n",
    "\n",
    "\n",
    "#### Architecture\n",
    "Is the structure of the network, e.g. number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
