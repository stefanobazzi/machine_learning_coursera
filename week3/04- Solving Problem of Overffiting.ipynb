{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem of Overfitting\n",
    "\n",
    "* Underfitting or \"high bias\": the model is too simple (e.g use linear model to represent a quadratic model)\n",
    "* Overfitting or \"high variance\": the model is too complex (e.g use fifth order polynomial model to represent a quadratic model)\n",
    "\n",
    "\n",
    "#### Addressing overfitting\n",
    "\n",
    "Options:\n",
    "1. Reduce number of features.\n",
    "    * Manulally select wich features to keep.\n",
    "    * Model selection algorithm (later in course)\n",
    "2. Regularization.\n",
    "    * Keep all features, but reduce magnitude/values of parameters $\\theta_j$.\n",
    "    * Works well when we have a lot of features, each of wich contributes a bit to predicting $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "#### Intuition\n",
    "The idea is to choose (or find) some parameters and make it small, so they contribute less to the hypothesis approximation.\n",
    "\n",
    "e.g \n",
    "* correct hypohtesis : $h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2$\n",
    "* current hypothesis : $h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 \\theta_3x^3 + \\theta_4x^4$\n",
    "\n",
    "We want to remove the influence of $\\theta_3$ and $\\theta_4$ terms and so add an addittional term to error function to \"force\" $\\theta_3$ and $\\theta_4$ to be small\n",
    "\n",
    "$ min \\frac{1}{2m}\\sum_{i=0}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 + 1000\\theta_3^2 + 1000\\theta_4^2$\n",
    "\n",
    "To get a low error  $\\theta_3$ and $\\theta_4$ will be about zero\n",
    "\n",
    "#### Regularization\n",
    "Small values for parameters $\\theta_0, \\theta_1, \\dots, \\theta_n$\n",
    "* Simpler hypothesis\n",
    "* Less prone to overfitting\n",
    "the regularuzation term gives a smoother shape to the hypothesis\n",
    "\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m}\\sum_{i=0}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{i=0}^{n}\\theta_j^2$\n",
    "\n",
    "$\\lambda\\sum_{i=0}^{n}\\theta_j^2$ : Regualarization parameter\n",
    "\n",
    "If the $\\lambda$ parameter is too large (e.g $\\lambda = 10^{10}$) the train goes to underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression\n",
    "\n",
    "#### Cost Function\n",
    "\n",
    "$ J(\\theta) =  -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(h_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1 - h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{i=1}^{n}\\theta_j^2 $ \n",
    "\n",
    "#### Gradient descent\n",
    "We will modify our gradient descent function to separate out θ0\\theta_0θ0​ from the rest of the parameters because we do not want to penalize $\\theta_0$.\n",
    "\n",
    "Repeat {\n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)}$\n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_j := \\theta_j - \\alpha[(\\sum_{i=0}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}) + \n",
    "\\frac{\\lambda}{m}{\\theta_j}]$ \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\theta_j \\in \\{1, 2, \\dots, n\\}$\n",
    "\n",
    "}\n",
    "\n",
    "Repeat {\n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)}$\n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_j := \\theta_j(1 - \\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m}\\sum_{i=0}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$ \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\theta_j \\in \\{1, 2, \\dots, n\\}$\n",
    "\n",
    "}\n",
    "\n",
    "$ (1 - \\alpha\\frac{\\lambda}{m}) $ will always be less than 1\n",
    "\n",
    "#### Normal Equation\n",
    "\n",
    "$\\theta = (X^TX +\\lambda  L)^{-1}X^Ty$\n",
    "\n",
    "where $ L = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "&1 \\\\\n",
    "&&1 \\\\\n",
    "&&&\\ddots \\\\\n",
    "&&&&1 \\\\\n",
    "\\end{bmatrix}$   &nbsp;&nbsp;&nbsp;&nbsp; $ (n + 1) * (n + 1) $\n",
    "\n",
    "Recall that if m < n, then $ X^TX $ is non-invertible. However, when we add the term $ \\lambda  L $, then $ X^TX +  \\lambda  L $  becomes invertible\n",
    "\n",
    "#### Advanced Optimization\n",
    "\n",
    "on Ocatve:\n",
    "\n",
    "function [jVal, gradient] = costFunction(theta)    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; jVal = code to compute $\\frac{\\partial }{\\partial \\theta_0}J(\\theta)$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ J(\\theta) =  -\\frac{1}{m}\\sum_{i=1}^{m}[y^{(i)}\\log(h_\\theta(x^{(i)})) + (1 - y^{(i)})\\log(1 - h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{i=1}^{n}\\theta_j^2 $ \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; gradient(1) = code to compute $\\frac{\\partial }{\\partial \\theta_0}J(\\theta)$   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; gradient(2) = code to compute $\\frac{\\partial }{\\partial \\theta_1}J(\\theta)$    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ \\alpha[(\\sum_{i=0}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_1^{(i)}) +  \\frac{\\lambda}{m}{\\theta_1}]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; gradient(3) = code to compute $\\frac{\\partial }{\\partial \\theta_2}J(\\theta)$    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ \\alpha[(\\sum_{i=0}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_1^{(i)}) +  \\frac{\\lambda}{m}{\\theta_2}]$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; gradient(n + 1) = code to compute $\\frac{\\partial }{\\partial \\theta_n}J(\\theta)$    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $ \\alpha[(\\sum_{i=0}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_1^{(i)}) +  \\frac{\\lambda}{m}{\\theta_n}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
