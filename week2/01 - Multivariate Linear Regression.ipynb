{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Feature\n",
    "\n",
    "Notation:   \n",
    "$n = $ number of faetures     \n",
    "$x^{i} = $ input (features) of $i^{th}$ example   \n",
    "$x_j^{(i)} = $ value of feature $j$ in the $i^{th}$ training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis\n",
    "\n",
    "For convenience of notation $x_0 = 1$  ($x_0^{(i)} = 1$)\n",
    "\n",
    "$h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + + \\theta_nx_n$\n",
    "\n",
    "$ x = \\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix} \\in \\mathbb{R}^{n+1}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\theta = \\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} \\in \\mathbb{R}^{n+1}$\n",
    "\n",
    " Mutivariate Regression   \n",
    "$h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + + \\theta_nx_n =\n",
    "\\begin{bmatrix}\\theta_0 & \\theta_1 & \\theta_2 & \\dots & \\theta_n \\end{bmatrix}\n",
    "\\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix}\n",
    "= \\theta^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: $h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + + \\theta_nx_n$\n",
    "\n",
    "Parameters: $\\theta_0 , \\theta_1 , \\theta_2 , \\dots , \\theta_n$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\theta$ is $n+1$ dimensional vector\n",
    "\n",
    "Cost Function: $J(\\theta_0, \\theta_1, \\dots, \\theta_n) = J(\\theta) = \\frac{1}{2m}\\sum_{i=0}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "#### Gradient Descent\n",
    "Repeat {\n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_j := \\theta_j - \\alpha\\frac{\\partial }{\\partial \\theta_j}J(\\theta_0, \\theta_1, \\dots, \\theta_n) $ \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; (for $j = 0$ and $j = 1$)\n",
    "\n",
    "} &nbsp;&nbsp;&nbsp;&nbsp; Simultaneously update for every $j = 0, \\dots, n$\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "Repeat {\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; for  $j = 0, \\dots, n$\n",
    "\n",
    "}\n",
    "\n",
    "Repeat {\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)}$ \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $(x_0 = 1)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_1 := \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_1^{(i)}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_2 := \\theta_2 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_2^{(i)}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\dots$\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: make sure features are on a similar scale   \n",
    "E.g.:   \n",
    "$x_1$ = size (0 -2000 feet²)   \n",
    "$x_2$ = number of bedrooms (1 -5)   \n",
    "\n",
    "Gradient descent in this case can take a lot of time to converge, so scaling the feature can help the gradient to converge much faster.\n",
    "\n",
    "$x_1 = \\frac{\\text{size (feet²)}}{2000}$ &nbsp;&nbsp;&nbsp;&nbsp; $x_2 = \\frac{\\text{number of bedrooms}}{5}$\n",
    "\n",
    "Get every feature approximately $-1 <= x_i <= 1$\n",
    "\n",
    "Suggested ranges:\n",
    "* $-3$ to $3$\n",
    "* $-\\frac{1}{3}$ to $\\frac{1}{3}$\n",
    "\n",
    "#### Mean noramalization\n",
    "Replaece $x_i$ with $x_i -\\mu_i$ to make features have approximately mean zero (Do not apply to $x_0 = 1$)\n",
    "\n",
    "$x_i = \\frac{x_i - \\mu_i}{s_i}$ &nbsp;&nbsp;&nbsp;&nbsp; $\\mu_i$ = average of all values &nbsp;&nbsp;&nbsp;&nbsp; $s_i = max -min$ (range of values or standard deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Debugging: how the make sure gradient descent is working correctly\n",
    "* Choose the right learnig rate\n",
    "\n",
    "$J(\\theta)$ should decrease after every iteration. Number of iterations to converge can be really different on the particular case e-g 30, 3000, 3000000.\n",
    "\n",
    "Example automatic convergence test: Declare convergence if $J(\\theta)$ decreases by less than $10^{-3}$ in one iteration\n",
    "\n",
    "if $J(\\theta)$ is increasing Gradient descent is not working correctly -> decrease $\\alpha$\n",
    "if $J(\\theta)$ is oscillating Gradient descent is not working correctly -> decrease $\\alpha$\n",
    "so if $\\alpha$ is too large $J(\\theta)$ may not decrease on every iteration, may not converge\n",
    "but if $\\alpha$ is too small gradiente descent can be slow to converge\n",
    "\n",
    "To choose $\\alpha$ try:\n",
    "$ \\dots, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1 \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Housing prices prediction\n",
    "\n",
    " $h_\\theta(x) = \\theta_0x_0 + \\theta_1 * \\text{frontage} + \\theta_2 * \\text{depth} $\n",
    " \n",
    " e.g define area = frontage x depth and use only one feature   \n",
    " $h_\\theta(x) = \\theta_0x_0 + \\theta_1 * \\text{area}$\n",
    " \n",
    " #### Polynomial regression\n",
    " \n",
    " Our hypothesis need to be not linear, We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).\n",
    " \n",
    " $h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2$\n",
    " \n",
    " $h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 +  \\theta_3x^3$\n",
    " \n",
    "  $h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 =\n",
    " \\theta_0 + \\theta_1(\\text{size}) + \\theta_2(\\text{size})^2 + \\theta_3(\\text{size})^3$\n",
    " \n",
    " If use plynomial regression quadratic or cubic feature can be really big numbers, remember feature scaling!\n",
    " \n",
    "  $\\theta_0 + \\theta_1(\\text{size}) + \\theta_2(\\text{size})^2$ quadratic function can also decrease! maybe a better choice can be $\\theta_0 + \\theta_1(\\text{size}) + \\theta_2\\sqrt{\\text{size}}$ \n",
    "  \n",
    "keep attention to choose the right choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
