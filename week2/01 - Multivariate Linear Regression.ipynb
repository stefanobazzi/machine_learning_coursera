{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Feature\n",
    "\n",
    "Notation:   \n",
    "$n = $ number of faetures     \n",
    "$x^{i} = $ input (features) of $i^{th}$ example   \n",
    "$x_j^{(i)} = $ value of feature $j$ in the $i^{th}$ training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis\n",
    "\n",
    "For convenience of notation $x_0 = 1$  ($x_0^{(i)} = 1$)\n",
    "\n",
    "$h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + + \\theta_nx_n$\n",
    "\n",
    "$ x = \\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix} \\in \\mathbb{R}^{n+1}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\theta = \\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} \\in \\mathbb{R}^{n+1}$\n",
    "\n",
    " Mutivariate Regression   \n",
    "$h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + + \\theta_nx_n =\n",
    "\\begin{bmatrix}\\theta_0 & \\theta_1 & \\theta_2 & \\dots & \\theta_n \\end{bmatrix}\n",
    "\\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix}\n",
    "= \\theta^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: $h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + + \\theta_nx_n$\n",
    "\n",
    "Parameters: $\\theta_0 , \\theta_1 , \\theta_2 , \\dots , \\theta_n$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\theta$ is $n+1$ dimensional vector\n",
    "\n",
    "Cost Function: $J(\\theta_0, \\theta_1, \\dots, \\theta_n) = J(\\theta) = \\frac{1}{2m}\\sum_{i=0}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "#### Gradient Descent\n",
    "Repeat {\n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_j := \\theta_j - \\alpha\\frac{\\partial }{\\partial \\theta_j}J(\\theta_0, \\theta_1, \\dots, \\theta_n) $ \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; (for $j = 0$ and $j = 1$)\n",
    "\n",
    "} &nbsp;&nbsp;&nbsp;&nbsp; Simultaneously update for every $j = 0, \\dots, n$\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "Repeat {\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; for  $j = 0, \\dots, n$\n",
    "\n",
    "}\n",
    "\n",
    "Repeat {\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)}$ \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $(x_0 = 1)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_1 := \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_1^{(i)}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_2 := \\theta_2 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_2^{(i)}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\dots$\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: make sure features are on a similar scale   \n",
    "E.g.:   \n",
    "$x_1$ = size (0 -2000 feet²)   \n",
    "$x_2$ = number of bedrooms (1 -5)   \n",
    "\n",
    "Gradient descent in this case can take a lot of time to converge, so scaling the feature can help the gradient to converge much faster.\n",
    "\n",
    "$x_1 = \\frac{\\text{size (feet²)}}{2000}$ &nbsp;&nbsp;&nbsp;&nbsp; $x_2 = \\frac{\\text{number of bedrooms}}{5}$\n",
    "\n",
    "Get every feature approximately $-1 \\leq x_i \\leq 1$\n",
    "\n",
    "Suggested ranges:\n",
    "* $-3$ to $3$\n",
    "* $-\\frac{1}{3}$ to $\\frac{1}{3}$\n",
    "\n",
    "#### Mean noramalization\n",
    "Replaece $x_i$ with $x_i -\\mu_i$ to make features have approximately mean zero (Do not apply to $x_0 = 1$)\n",
    "\n",
    "$x_i = \\frac{x_i - \\mu_i}{s_i}$ &nbsp;&nbsp;&nbsp;&nbsp; $\\mu_i$ = average of all values &nbsp;&nbsp;&nbsp;&nbsp; $s_i = max -min$ (range of values or standard deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Debugging: how the make sure gradient descent is working correctly\n",
    "* Choose the right learnig rate\n",
    "\n",
    "$J(\\theta)$ should decrease after every iteration. Number of iterations to converge can be really different on the particular case e-g 30, 3000, 3000000.\n",
    "\n",
    "Example automatic convergence test: Declare convergence if $J(\\theta)$ decreases by less than $10^{-3}$ in one iteration\n",
    "\n",
    "if $J(\\theta)$ is increasing Gradient descent is not working correctly -> decrease $\\alpha$\n",
    "if $J(\\theta)$ is oscillating Gradient descent is not working correctly -> decrease $\\alpha$\n",
    "so if $\\alpha$ is too large $J(\\theta)$ may not decrease on every iteration, may not converge\n",
    "but if $\\alpha$ is too small gradiente descent can be slow to converge\n",
    "\n",
    "To choose $\\alpha$ try:\n",
    "$ \\dots, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1 \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Housing prices prediction\n",
    "\n",
    " $h_\\theta(x) = \\theta_0x_0 + \\theta_1 * \\text{frontage} + \\theta_2 * \\text{depth} $\n",
    " \n",
    " e.g define area = frontage x depth and use only one feature   \n",
    " $h_\\theta(x) = \\theta_0x_0 + \\theta_1 * \\text{area}$\n",
    " \n",
    " #### Polynomial regression\n",
    " \n",
    " Our hypothesis need to be not linear, We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).\n",
    " \n",
    " $h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2$\n",
    " \n",
    " $h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 +  \\theta_3x^3$\n",
    " \n",
    "  $h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 =\n",
    " \\theta_0 + \\theta_1(\\text{size}) + \\theta_2(\\text{size})^2 + \\theta_3(\\text{size})^3$\n",
    " \n",
    " If use plynomial regression quadratic or cubic feature can be really big numbers, remember feature scaling!\n",
    " \n",
    "  $\\theta_0 + \\theta_1(\\text{size}) + \\theta_2(\\text{size})^2$ quadratic function can also decrease! maybe a better choice can be $\\theta_0 + \\theta_1(\\text{size}) + \\theta_2\\sqrt{\\text{size}}$ \n",
    "  \n",
    "keep attention to choose the right choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14268679 0.68542105]\n",
      "[0.15098266 0.73461887]\n",
      "[0.15773623 0.77520418]\n",
      "[0.16321822 0.80868738]\n",
      "[0.16765185 0.83631398]\n",
      "[0.17122122 0.85911105]\n",
      "[0.17407811 0.87792553]\n",
      "[0.17634772 0.89345585]\n",
      "[0.17813327 0.90627792]\n",
      "[0.17951986 0.91686668]\n",
      "[0.1805777  0.92561375]\n",
      "[0.18136467 0.93284208]\n",
      "[0.18192849 0.93881799]\n",
      "[0.18230852 0.94376107]\n",
      "[0.18253722 0.94785244]\n",
      "[0.18264135 0.9512414 ]\n",
      "[0.18264298 0.95405113]\n",
      "[0.1825603  0.95638314]\n",
      "[0.18240832 0.9583212 ]\n",
      "[0.18219942 0.95993435]\n",
      "[0.18194379 0.96127953]\n",
      "[0.18164986 0.96240369]\n",
      "[0.18132455 0.96334557]\n",
      "[0.18097359 0.96413709]\n",
      "[0.18060168 0.96480457]\n",
      "[0.18021273 0.96536975]\n",
      "[0.17980994 0.96585051]\n",
      "[0.17939594 0.96626162]\n",
      "[0.17897293 0.96661527]\n",
      "[0.17854269 0.96692148]\n",
      "[0.1781067  0.96718855]\n",
      "[0.17766619 0.9674233 ]\n",
      "[0.17722216 0.96763135]\n",
      "[0.17677544 0.96781736]\n",
      "[0.17632672 0.96798514]\n",
      "[0.17587655 0.96813787]\n",
      "[0.1754254  0.96827813]\n",
      "[0.17497366 0.96840809]\n",
      "[0.17452162 0.9685295 ]\n",
      "[0.17406957 0.96864384]\n",
      "[0.17361769 0.9687523 ]\n",
      "[0.17316618 0.96885588]\n",
      "[0.17271518 0.96895539]\n",
      "[0.1722648  0.96905152]\n",
      "[0.17181513 0.96914482]\n",
      "[0.17136627 0.96923575]\n",
      "[0.17091826 0.96932468]\n",
      "[0.17047118 0.96941194]\n",
      "[0.17002505 0.96949777]\n",
      "[0.16957992 0.9695824 ]\n",
      "[0.1691358  0.96966599]\n",
      "[0.16869274 0.9697487 ]\n",
      "[0.16825074 0.96983064]\n",
      "[0.16780981 0.96991191]\n",
      "[0.16736997 0.96999259]\n",
      "[0.16693124 0.97007275]\n",
      "[0.1664936  0.97015245]\n",
      "[0.16605708 0.97023172]\n",
      "[0.16562167 0.97031062]\n",
      "[0.16518738 0.97038916]\n",
      "[0.16475421 0.97046738]\n",
      "[0.16432215 0.9705453 ]\n",
      "[0.16389122 0.97062294]\n",
      "[0.1634614 0.9707003]\n",
      "[0.1630327  0.97077741]\n",
      "[0.16260511 0.97085427]\n",
      "[0.16217865 0.97093089]\n",
      "[0.16175329 0.97100728]\n",
      "[0.16132905 0.97108344]\n",
      "[0.16090591 0.97115939]\n",
      "[0.16048388 0.97123511]\n",
      "[0.16006296 0.97131063]\n",
      "[0.15964313 0.97138593]\n",
      "[0.15922441 0.97146103]\n",
      "[0.15880679 0.97153592]\n",
      "[0.15839025 0.97161061]\n",
      "[0.15797481 0.9716851 ]\n",
      "[0.15756046 0.97175939]\n",
      "[0.1571472  0.97183348]\n",
      "[0.15673501 0.97190737]\n",
      "[0.15632391 0.97198107]\n",
      "[0.15591389 0.97205457]\n",
      "[0.15550494 0.97212788]\n",
      "[0.15509707 0.97220099]\n",
      "[0.15469026 0.97227391]\n",
      "[0.15428452 0.97234664]\n",
      "[0.15387984 0.97241917]\n",
      "[0.15347623 0.97249152]\n",
      "[0.15307368 0.97256367]\n",
      "[0.15267218 0.97263564]\n",
      "[0.15227173 0.97270741]\n",
      "[0.15187233 0.972779  ]\n",
      "[0.15147398 0.9728504 ]\n",
      "[0.15107668 0.97292161]\n",
      "[0.15068042 0.97299264]\n",
      "[0.1502852  0.97306348]\n",
      "[0.14989101 0.97313413]\n",
      "[0.14949786 0.9732046 ]\n",
      "[0.14910574 0.97327488]\n",
      "[0.14871464 0.97334498]\n",
      "[0.14832458 0.97341489]\n",
      "[0.14793553 0.97348462]\n",
      "[0.14754751 0.97355417]\n",
      "[0.14716051 0.97362354]\n",
      "[0.14677452 0.97369272]\n",
      "[0.14638954 0.97376172]\n",
      "[0.14600557 0.97383054]\n",
      "[0.14562261 0.97389918]\n",
      "[0.14524065 0.97396764]\n",
      "[0.1448597  0.97403593]\n",
      "[0.14447974 0.97410403]\n",
      "[0.14410078 0.97417195]\n",
      "[0.14372282 0.9742397 ]\n",
      "[0.14334585 0.97430726]\n",
      "[0.14296986 0.97437465]\n",
      "[0.14259486 0.97444187]\n",
      "[0.14222085 0.9745089 ]\n",
      "[0.14184781 0.97457576]\n",
      "[0.14147576 0.97464245]\n",
      "[0.14110468 0.97470896]\n",
      "[0.14073457 0.9747753 ]\n",
      "[0.14036544 0.97484146]\n",
      "[0.13999727 0.97490745]\n",
      "[0.13963007 0.97497326]\n",
      "[0.13926383 0.97503891]\n",
      "[0.13889855 0.97510438]\n",
      "[0.13853423 0.97516968]\n",
      "[0.13817087 0.9752348 ]\n",
      "[0.13780846 0.97529976]\n",
      "[0.137447   0.97536455]\n",
      "[0.13708648 0.97542917]\n",
      "[0.13672692 0.97549361]\n",
      "[0.13636829 0.97555789]\n",
      "[0.13601061 0.975622  ]\n",
      "[0.13565387 0.97568594]\n",
      "[0.13529806 0.97574972]\n",
      "[0.13494318 0.97581332]\n",
      "[0.13458924 0.97587676]\n",
      "[0.13423622 0.97594004]\n",
      "[0.13388413 0.97600314]\n",
      "[0.13353296 0.97606608]\n",
      "[0.13318271 0.97612886]\n",
      "[0.13283339 0.97619147]\n",
      "[0.13248498 0.97625392]\n",
      "[0.13213748 0.97631621]\n",
      "[0.13179089 0.97637833]\n",
      "[0.13144522 0.97644028]\n",
      "[0.13110045 0.97650208]\n",
      "[0.13075658 0.97656371]\n",
      "[0.13041362 0.97662518]\n",
      "[0.13007155 0.97668649]\n",
      "[0.12973038 0.97674764]\n",
      "[0.12939011 0.97680863]\n",
      "[0.12905073 0.97686946]\n",
      "[0.12871224 0.97693013]\n",
      "[0.12837464 0.97699064]\n",
      "[0.12803792 0.97705099]\n",
      "[0.12770209 0.97711119]\n",
      "[0.12736714 0.97717122]\n",
      "[0.12703306 0.9772311 ]\n",
      "[0.12669987 0.97729082]\n",
      "[0.12636754 0.97735039]\n",
      "[0.12603609 0.97740979]\n",
      "[0.12570551 0.97746905]\n",
      "[0.12537579 0.97752814]\n",
      "[0.12504694 0.97758708]\n",
      "[0.12471895 0.97764587]\n",
      "[0.12439183 0.97770451]\n",
      "[0.12406556 0.97776298]\n",
      "[0.12374014 0.97782131]\n",
      "[0.12341558 0.97787948]\n",
      "[0.12309187 0.9779375 ]\n",
      "[0.12276901 0.97799537]\n",
      "[0.122447   0.97805309]\n",
      "[0.12212583 0.97811065]\n",
      "[0.1218055  0.97816807]\n",
      "[0.12148602 0.97822533]\n",
      "[0.12116737 0.97828244]\n",
      "[0.12084956 0.97833941]\n",
      "[0.12053258 0.97839622]\n",
      "[0.12021643 0.97845289]\n",
      "[0.11990111 0.9785094 ]\n",
      "[0.11958662 0.97856577]\n",
      "[0.11927296 0.97862199]\n",
      "[0.11896011 0.97867806]\n",
      "[0.11864809 0.97873399]\n",
      "[0.11833689 0.97878977]\n",
      "[0.1180265 0.9788454]\n",
      "[0.11771692 0.97890089]\n",
      "[0.11740816 0.97895623]\n",
      "[0.11710021 0.97901143]\n",
      "[0.11679307 0.97906648]\n",
      "[0.11648673 0.97912138]\n",
      "[0.11618119 0.97917615]\n",
      "[0.11587646 0.97923077]\n",
      "[0.11557252 0.97928524]\n",
      "[0.11526939 0.97933958]\n",
      "[0.11496704 0.97939377]\n",
      "[0.11466549 0.97944781]\n",
      "[0.11436474 0.97950172]\n",
      "[0.11406477 0.97955549]\n",
      "0.0022014081520363668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f24a2cb6cc0>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def h(theta, x):\n",
    "    return np.dot(theta, x)\n",
    "\n",
    "def gradient_descent(x, y, theta, alpha=0.01):\n",
    "    _theta = []\n",
    "    for i, t in enumerate(theta):\n",
    "        gradient = np.sum((h(theta, x) - y)*x[i] for x, y in zip(x, y))\n",
    "        new = t - alpha / len(x) * gradient\n",
    "        _theta.append(new)\n",
    "    return np.array(_theta)\n",
    "\n",
    "def error(theta, x, y):\n",
    "    return np.sum([(h(theta, x) - y)**2 for x, y in zip(x, y)]) / (2 *len(x))\n",
    "\n",
    "\n",
    "data = np.array([[1, 1], [1, 2], [1, 4], [1, 8]])\n",
    "y = [1, 2, 4, 8]\n",
    "theta = np.random.rand(2)\n",
    "errors = []\n",
    "#data = np.hstack((np.ones((len(data), 1)), data))\n",
    "print(theta)\n",
    "for n in range(200):\n",
    "    theta = gradient_descent(data, y, theta, 0.008)\n",
    "    err = error(theta, data, y)\n",
    "    errors.append(err)\n",
    "    print(theta)\n",
    "print(err)\n",
    "plt.plot(errors)\n",
    "#plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
