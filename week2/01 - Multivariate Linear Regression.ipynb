{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Feature\n",
    "\n",
    "Notation:   \n",
    "$n = $ number of faetures     \n",
    "$x^{i} = $ input (features) of $i^{th}$ example   \n",
    "$x_j^{(i)} = $ value of feature $j$ in the $i^{th}$ training example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis\n",
    "\n",
    "For convenience of notation $x_0 = 1$  ($x_0^{(i)} = 1$)\n",
    "\n",
    "$h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + + \\theta_nx_n$\n",
    "\n",
    "$ x = \\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix} \\in \\mathbb{R}^{n+1}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\theta = \\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix} \\in \\mathbb{R}^{n+1}$\n",
    "\n",
    " Mutivariate Regression   \n",
    "$h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + + \\theta_nx_n =\n",
    "\\begin{bmatrix}\\theta_0 & \\theta_1 & \\theta_2 & \\dots & \\theta_n \\end{bmatrix}\n",
    "\\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix}\n",
    "= \\theta^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: $h_\\theta(x) = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + + \\theta_nx_n$\n",
    "\n",
    "Parameters: $\\theta_0 , \\theta_1 , \\theta_2 , \\dots , \\theta_n$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\theta$ is $n+1$ dimensional vector\n",
    "\n",
    "Cost Function: $J(\\theta_0, \\theta_1, \\dots, \\theta_n) = J(\\theta) = \\frac{1}{2m}\\sum_{i=0}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "#### Gradient Descent\n",
    "Repeat {\n",
    " \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_j := \\theta_j - \\alpha\\frac{\\partial }{\\partial \\theta_j}J(\\theta_0, \\theta_1, \\dots, \\theta_n) $ \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; (for $j = 0$ and $j = 1$)\n",
    "\n",
    "} &nbsp;&nbsp;&nbsp;&nbsp; Simultaneously update for every $j = 0, \\dots, n$\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "Repeat {\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; for  $j = 0, \\dots, n$\n",
    "\n",
    "}\n",
    "\n",
    "Repeat {\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)}$ \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $(x_0 = 1)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_1 := \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_1^{(i)}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_2 := \\theta_2 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_2^{(i)}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\dots$\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: make sure features are on a similar scale   \n",
    "E.g.:   \n",
    "$x_1$ = size (0 -2000 feet²)   \n",
    "$x_2$ = number of bedrooms (1 -5)   \n",
    "\n",
    "Gradient descent in this case can take a lot of time to converge, so scaling the feature can help the gradient to converge much faster.\n",
    "\n",
    "$x_1 = \\frac{\\text{size (feet²)}}{2000}$ &nbsp;&nbsp;&nbsp;&nbsp; $x_2 = \\frac{\\text{number of bedrooms}}{5}$\n",
    "\n",
    "Get every feature approximately $-1 \\leq x_i \\leq 1$\n",
    "\n",
    "Suggested ranges:\n",
    "* $-3$ to $3$\n",
    "* $-\\frac{1}{3}$ to $\\frac{1}{3}$\n",
    "\n",
    "#### Mean noramalization\n",
    "Replaece $x_i$ with $x_i -\\mu_i$ to make features have approximately mean zero (Do not apply to $x_0 = 1$)\n",
    "\n",
    "$x_i = \\frac{x_i - \\mu_i}{s_i}$ &nbsp;&nbsp;&nbsp;&nbsp; $\\mu_i$ = average of all values &nbsp;&nbsp;&nbsp;&nbsp; $s_i = max -min$ (range of values or standard deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Debugging: how the make sure gradient descent is working correctly\n",
    "* Choose the right learnig rate\n",
    "\n",
    "$J(\\theta)$ should decrease after every iteration. Number of iterations to converge can be really different on the particular case e-g 30, 3000, 3000000.\n",
    "\n",
    "Example automatic convergence test: Declare convergence if $J(\\theta)$ decreases by less than $10^{-3}$ in one iteration\n",
    "\n",
    "if $J(\\theta)$ is increasing Gradient descent is not working correctly -> decrease $\\alpha$\n",
    "if $J(\\theta)$ is oscillating Gradient descent is not working correctly -> decrease $\\alpha$\n",
    "so if $\\alpha$ is too large $J(\\theta)$ may not decrease on every iteration, may not converge\n",
    "but if $\\alpha$ is too small gradiente descent can be slow to converge\n",
    "\n",
    "To choose $\\alpha$ try:\n",
    "$ \\dots, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1 \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Housing prices prediction\n",
    "\n",
    " $h_\\theta(x) = \\theta_0x_0 + \\theta_1 * \\text{frontage} + \\theta_2 * \\text{depth} $\n",
    " \n",
    " e.g define area = frontage x depth and use only one feature   \n",
    " $h_\\theta(x) = \\theta_0x_0 + \\theta_1 * \\text{area}$\n",
    " \n",
    " #### Polynomial regression\n",
    " \n",
    " Our hypothesis need to be not linear, We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).\n",
    " \n",
    " $h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2$\n",
    " \n",
    " $h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 +  \\theta_3x^3$\n",
    " \n",
    "  $h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 =\n",
    " \\theta_0 + \\theta_1(\\text{size}) + \\theta_2(\\text{size})^2 + \\theta_3(\\text{size})^3$\n",
    " \n",
    " If use plynomial regression quadratic or cubic feature can be really big numbers, remember feature scaling!\n",
    " \n",
    "  $\\theta_0 + \\theta_1(\\text{size}) + \\theta_2(\\text{size})^2$ quadratic function can also decrease! maybe a better choice can be $\\theta_0 + \\theta_1(\\text{size}) + \\theta_2\\sqrt{\\text{size}}$ \n",
    "  \n",
    "keep attention to choose the right choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60687242 0.26680614]\n",
      "[0.62401326 0.37324292]\n",
      "[0.63782386 0.46107123]\n",
      "[0.64888913 0.5335544 ]\n",
      "[0.65769139 0.59338348]\n",
      "[0.66462835 0.64277755]\n",
      "[0.670028   0.68356651]\n",
      "[0.67416078 0.71725937]\n",
      "[0.67724971 0.74510045]\n",
      "[0.6794787  0.76811588]\n",
      "[0.6809994  0.78715182]\n",
      "[0.68193685 0.80290603]\n",
      "[0.68239417 0.8159539 ]\n",
      "[0.6824564  0.82676991]\n",
      "[0.68219365 0.83574533]\n",
      "[0.68166374 0.84320282]\n",
      "[0.68091435 0.84940843]\n",
      "[0.67998478 0.85458156]\n",
      "[0.67890746 0.85890315]\n",
      "[0.6777091  0.86252239]\n",
      "[0.67641176 0.86556231]\n",
      "[0.67503359 0.86812437]\n",
      "[0.67358959 0.87029222]\n",
      "[0.67209211 0.87213485]\n",
      "[0.67055133 0.87370916]\n",
      "[0.66897564 0.87506207]\n",
      "[0.66737197 0.87623225]\n",
      "[0.66574603 0.87725161]\n",
      "[0.66410251 0.87814645]\n",
      "[0.6624453  0.87893848]\n",
      "[0.66077758 0.87964558]\n",
      "[0.659102  0.8802825]\n",
      "[0.65742071 0.88086142]\n",
      "[0.6557355  0.88139236]\n",
      "[0.65404784 0.88188359]\n",
      "[0.65235895 0.88234194]\n",
      "[0.65066982 0.88277305]\n",
      "[0.64898127 0.88318153]\n",
      "[0.64729398 0.88357123]\n",
      "[0.64560849 0.88394531]\n",
      "[0.64392526 0.88430635]\n",
      "[0.64224467 0.88465651]\n",
      "[0.64056701 0.88499756]\n",
      "[0.63889255 0.88533097]\n",
      "[0.63722148 0.88565793]\n",
      "[0.63555397 0.88597943]\n",
      "[0.63389016 0.88629631]\n",
      "[0.63223015 0.88660923]\n",
      "[0.63057403 0.88691876]\n",
      "[0.62892187 0.88722535]\n",
      "[0.62727374 0.88752938]\n",
      "[0.62562967 0.88783118]\n",
      "[0.62398969 0.88813099]\n",
      "[0.62235385 0.88842903]\n",
      "[0.62072215 0.88872548]\n",
      "[0.6190946  0.88902048]\n",
      "[0.61747123 0.88931416]\n",
      "[0.61585204 0.88960662]\n",
      "[0.61423702 0.88989793]\n",
      "[0.61262619 0.89018817]\n",
      "[0.61101953 0.8904774 ]\n",
      "[0.60941706 0.89076565]\n",
      "[0.60781875 0.89105298]\n",
      "[0.60622461 0.89133941]\n",
      "[0.60463463 0.89162497]\n",
      "[0.60304881 0.89190969]\n",
      "[0.60146712 0.89219358]\n",
      "[0.59988958 0.89247666]\n",
      "[0.59831616 0.89275894]\n",
      "[0.59674687 0.89304043]\n",
      "[0.59518168 0.89332115]\n",
      "[0.59362059 0.89360111]\n",
      "[0.59206359 0.8938803 ]\n",
      "[0.59051067 0.89415874]\n",
      "[0.58896183 0.89443644]\n",
      "[0.58741704 0.89471339]\n",
      "[0.5858763 0.8949896]\n",
      "[0.5843396  0.89526508]\n",
      "[0.58280693 0.89553983]\n",
      "[0.58127828 0.89581385]\n",
      "[0.57975364 0.89608715]\n",
      "[0.578233   0.89635972]\n",
      "[0.57671634 0.89663158]\n",
      "[0.57520366 0.89690272]\n",
      "[0.57369495 0.89717315]\n",
      "[0.5721902  0.89744286]\n",
      "[0.57068939 0.89771187]\n",
      "[0.56919252 0.89798017]\n",
      "[0.56769958 0.89824777]\n",
      "[0.56621055 0.89851466]\n",
      "[0.56472542 0.89878085]\n",
      "[0.56324419 0.89904634]\n",
      "[0.56176685 0.89931114]\n",
      "[0.56029338 0.89957524]\n",
      "[0.55882378 0.89983865]\n",
      "[0.55735803 0.90010136]\n",
      "[0.55589612 0.90036339]\n",
      "[0.55443805 0.90062473]\n",
      "[0.5529838  0.90088539]\n",
      "[0.55153337 0.90114536]\n",
      "[0.55008674 0.90140464]\n",
      "[0.54864391 0.90166325]\n",
      "[0.54720486 0.90192118]\n",
      "[0.54576959 0.90217844]\n",
      "[0.54433808 0.90243501]\n",
      "[0.54291032 0.90269092]\n",
      "[0.54148631 0.90294615]\n",
      "[0.54006604 0.90320072]\n",
      "[0.53864949 0.90345461]\n",
      "[0.53723665 0.90370785]\n",
      "[0.53582752 0.90396041]\n",
      "[0.53442209 0.90421232]\n",
      "[0.53302035 0.90446356]\n",
      "[0.53162228 0.90471414]\n",
      "[0.53022787 0.90496407]\n",
      "[0.52883713 0.90521334]\n",
      "[0.52745003 0.90546196]\n",
      "[0.52606657 0.90570993]\n",
      "[0.52468674 0.90595724]\n",
      "[0.52331053 0.90620391]\n",
      "[0.52193793 0.90644993]\n",
      "[0.52056893 0.9066953 ]\n",
      "[0.51920352 0.90694003]\n",
      "[0.51784169 0.90718412]\n",
      "[0.51648343 0.90742757]\n",
      "[0.51512874 0.90767038]\n",
      "[0.5137776  0.90791255]\n",
      "[0.51243    0.90815409]\n",
      "[0.51108594 0.908395  ]\n",
      "[0.5097454  0.90863527]\n",
      "[0.50840838 0.90887491]\n",
      "[0.50707486 0.90911393]\n",
      "[0.50574485 0.90935231]\n",
      "[0.50441832 0.90959007]\n",
      "[0.50309527 0.90982721]\n",
      "[0.50177569 0.91006373]\n",
      "[0.50045957 0.91029962]\n",
      "[0.49914691 0.9105349 ]\n",
      "[0.49783769 0.91076956]\n",
      "[0.4965319 0.9110036]\n",
      "[0.49522953 0.91123703]\n",
      "[0.49393059 0.91146985]\n",
      "[0.49263505 0.91170206]\n",
      "[0.4913429  0.91193366]\n",
      "[0.49005415 0.91216465]\n",
      "[0.48876878 0.91239503]\n",
      "[0.48748678 0.91262482]\n",
      "[0.48620814 0.91285399]\n",
      "[0.48493285 0.91308257]\n",
      "[0.48366091 0.91331055]\n",
      "[0.48239231 0.91353793]\n",
      "[0.48112703 0.91376471]\n",
      "[0.47986508 0.9139909 ]\n",
      "[0.47860643 0.91421649]\n",
      "[0.47735108 0.9144415 ]\n",
      "[0.47609903 0.91466591]\n",
      "[0.47485026 0.91488973]\n",
      "[0.47360476 0.91511297]\n",
      "[0.47236254 0.91533562]\n",
      "[0.47112357 0.91555769]\n",
      "[0.46988785 0.91577918]\n",
      "[0.46865537 0.91600008]\n",
      "[0.46742613 0.91622041]\n",
      "[0.4662001  0.91644015]\n",
      "[0.4649773  0.91665932]\n",
      "[0.4637577  0.91687792]\n",
      "[0.4625413  0.91709594]\n",
      "[0.46132809 0.91731339]\n",
      "[0.46011807 0.91753027]\n",
      "[0.45891121 0.91774659]\n",
      "[0.45770753 0.91796233]\n",
      "[0.456507   0.91817751]\n",
      "[0.45530962 0.91839212]\n",
      "[0.45411537 0.91860617]\n",
      "[0.45292427 0.91881966]\n",
      "[0.45173628 0.91903259]\n",
      "[0.45055141 0.91924496]\n",
      "[0.44936965 0.91945678]\n",
      "[0.44819099 0.91966803]\n",
      "[0.44701542 0.91987874]\n",
      "[0.44584294 0.92008889]\n",
      "[0.44467353 0.92029849]\n",
      "[0.44350719 0.92050754]\n",
      "[0.4423439  0.92071604]\n",
      "[0.44118367 0.920924  ]\n",
      "[0.44002648 0.92113141]\n",
      "[0.43887233 0.92133828]\n",
      "[0.4377212 0.9215446]\n",
      "[0.43657309 0.92175038]\n",
      "[0.435428   0.92195562]\n",
      "[0.4342859  0.92216033]\n",
      "[0.43314681 0.92236449]\n",
      "[0.4320107  0.92256813]\n",
      "[0.43087757 0.92277122]\n",
      "[0.42974741 0.92297379]\n",
      "[0.42862022 0.92317582]\n",
      "[0.42749598 0.92337733]\n",
      "[0.42637469 0.9235783 ]\n",
      "[0.42525635 0.92377875]\n",
      "[0.42414093 0.92397867]\n",
      "[0.42302845 0.92417807]\n",
      "0.030278662325644787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4ec536cf28>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def h(theta, x):\n",
    "    return np.dot(theta, x)\n",
    "\n",
    "def gradient_descent(x, y, theta, alpha=0.01):\n",
    "    _theta = []\n",
    "    for i, t in enumerate(theta):\n",
    "        gradient = np.sum((h(theta, x) - y)*x[i] for x, y in zip(x, y))\n",
    "        new = t - alpha / len(x) * gradient\n",
    "        _theta.append(new)\n",
    "    return np.array(_theta)\n",
    "\n",
    "def error(theta, x, y):\n",
    "    return np.sum([(h(theta, x) - y)**2 for x, y in zip(x, y)]) / (2 *len(x))\n",
    "\n",
    "\n",
    "data = np.array([[1, 1], [1, 2], [1, 4], [1, 8]])\n",
    "y = [1, 2, 4, 8]\n",
    "theta = np.random.rand(2)\n",
    "errors = []\n",
    "#data = np.hstack((np.ones((len(data), 1)), data))\n",
    "print(theta)\n",
    "for n in range(200):\n",
    "    theta = gradient_descent(data, y, theta, 0.008)\n",
    "    err = error(theta, data, y)\n",
    "    errors.append(err)\n",
    "    print(theta)\n",
    "print(err)\n",
    "plt.plot(errors)\n",
    "#plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
