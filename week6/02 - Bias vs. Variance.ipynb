{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosis Bias vs. Variance\n",
    "\n",
    "- High bias (underfit)\n",
    "- Just Right\n",
    "- High variance (overfit)\n",
    "\n",
    "Trainig error: $ J_{train}(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 $  \n",
    "Cross validation error: $ J_{cv}(\\theta) = \\frac{1}{2m_{cv}}\\sum_{i=1}^{m_{cv}}(h_\\theta(x_{cv}^{(i)}) - y_{cv}^{(i)})^2 $\n",
    "\n",
    "Increasing the degree of the polynomial we fite better and the train error decrease, but if the model is overfit the cross validation error will increase.\n",
    "\n",
    "Bias (underfit) \n",
    "- $ J_{train}(\\theta)$ will be high\n",
    "- $ J_{cv}(\\theta) \\approx J_{train}(\\theta)$\n",
    "\n",
    "Variance (overfit)\n",
    "- $ J_{train}(\\theta)$ will be low\n",
    "- $ J_{cv}(\\theta) \\gg J_{train}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization and Bias/Variance\n",
    "\n",
    " Model: $ h_{\\theta}(x)  = \\theta_0 + \\theta_1x + \\theta_2x^{(2)} + \\theta_3x^{(3)}  + \\theta_4x^{(4)}$   \n",
    " \n",
    " $ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\frac{\\lambda}{2m}\\sum_{i=1}^{m}\\theta_j^2 $ \n",
    " \n",
    " - Largee $\\lambda$ -> high bias (underfit) \n",
    " - Just right $\\lambda$\n",
    " - Small $\\lambda$ -> variance (overfit)\n",
    " \n",
    " #### Choosing the regularization parameter $\\lambda$ \n",
    " \n",
    " \n",
    "1. Create a list of lambdas (i.e. $\\lambda \\in${0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24});\n",
    "2. Create a set of models with different degrees or any other variants.\n",
    "3. Iterate through the $\\lambda$ and for each $\\lambda$ go through all the models to learn some $\\Theta$.\n",
    "4. Compute the cross validation error using the learned Θ (computed with λ) on the $J_{CV}(\\Theta)$ without regularization or $\\lambda = 0$.\n",
    "5. Select the best combo that produces the lowest error on the cross validation set.\n",
    "6. Using the best combo Θ and λ, apply it on $J_{test}(\\Theta)$ to see if it has a good generalization of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves\n",
    "\n",
    "Tool for diagnose bias/varinace, reduce the size of m and\n",
    "\n",
    "- plot $ J_{train}(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2 $  \n",
    "- plot $ J_{cv}(\\theta) = \\frac{1}{2m_{cv}}\\sum_{i=1}^{m_{cv}}(h_\\theta(x_{cv}^{(i)}) - y_{cv}^{(i)})^2$\n",
    "\n",
    "plot a graph with error values on gradually increasig the size of m\n",
    "\n",
    "- small m -> error is small it's easy to find the right model\n",
    "- big m -> error is bigger \n",
    "\n",
    "#### High bias\n",
    "if a learning algorithm is suffering from high bias, getting more trainig data will not (by itself) help much.\n",
    "\n",
    "- small m -> train error is small and cross validation error is big\n",
    "- big m ->  the errors are quite the same \n",
    "\n",
    "usuallly the desired performance is lower than the current\n",
    "\n",
    "#### High variance\n",
    "\n",
    "if a learning algorithm is suffering from high varinace, getting more trainig data is likely to help.\n",
    "\n",
    "- small m -> train error is small and cross validation error is big\n",
    "- big m ->  there is a difference beetween the two errors, cross validation error is much larger than trainig error\n",
    "\n",
    "usuallly the desired performance is beetween the current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decideng what to do next Revisited\n",
    "\n",
    "Debugging a learning algorithm:\n",
    "\n",
    "- Getting more training examples: Fixes high variance\n",
    "- Trying smaller sets of features: Fixes high variance\n",
    "- Adding features: Fixes high bias\n",
    "- Adding polynomial features: Fixes high bias\n",
    "- Decreasing  $\\lambda$ : Fixes high bias\n",
    "- Increasing  $\\lambda$ : Fixes high variance.\n",
    "\n",
    "\n",
    "#### Neural network and overfitting\n",
    "\n",
    "\"Small\" neural network:\n",
    "- fewer parameters, more proone underfitting\n",
    "- computationaly cheaper\n",
    "\n",
    "\"Large\" neural network:\n",
    "- more parameters, more proone overfitting\n",
    "- computationaly more expensive\n",
    "- need regularization to address overfitting\n",
    "\n",
    "Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best. \n",
    "\n",
    "#### Model Complexity Effects:\n",
    "\n",
    "- Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.\n",
    "- Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.\n",
    "- In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
