{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemetation note: Unrolling Parameters\n",
    "\n",
    "#### Advanced Optimization\n",
    "Neural Network (L=4)   \n",
    "* $\\Theta^{(1)}, \\Theta^{(2)}, \\Theta^{(3)}$ - matrices (Theta1, Theat2, Theta3)\n",
    "* $D^{(1)}, D^{(2)}, D^{(3)} $ - matrices\n",
    "\n",
    "\"unroll\" into vectors \n",
    "\n",
    "\n",
    "#### Example\n",
    "\n",
    "$s_1 = 10, s_2 = 10, s_3 = 1$\n",
    "\n",
    "$\\Theta^{(1)} \\in \\mathbb{R}^{10 x 11}, \\Theta^{(2)} \\in \\mathbb{R}^{10 x 11}, \\Theta^{(3)} \\in \\mathbb{R}^{1 x 11}$    \n",
    "$D^{(1)} \\in \\mathbb{R}^{10 x 11}, D^{(2)} \\in \\mathbb{R}^{10 x 11}, D^{(3)} \\in \\mathbb{R}^{1 x 11}$\n",
    "\n",
    "thetaVec = [ Theta1(:); Theta2(:); Theta3(:)];   \n",
    "DVec = [ D1(:); D2(:); D3(:)];   \n",
    "\n",
    "Theta1 = reshape(thetaVec(1:110), 10, 11);\n",
    "Theta2 = reshape(thetaVec(111:220), 10, 11);\n",
    "Theta3 = reshape(thetaVec(221:231), 1, 11);\n",
    "\n",
    "Unrll to get initialTheta to  pass to fminun(@costFuction, initalTheta, options)    \n",
    "\n",
    "function [jVal, gradientVec] = costFunction(thetaVec)      \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; From thetaVec get $\\Theta^{(1)}, \\Theta^{(2)},\\Theta^{(3)}$        \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; User foward prop / back prop ot compute $D^{(1)}, D^{(2)}, D^{(3)}$ and $J(\\Theta)$     \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Unroll  $D^{(1)}, D^{(2)}, D^{(3)}$  to get gradientVec.    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Checking\n",
    "\n",
    "Userful to check if the gradient is implemeted correctly\n",
    "\n",
    "Suppose to have a function $J(\\Theta)$  , consider to calcualte the derivative in one point $\\theta$ we get the slope of the function. It can be approximate to:    \n",
    "\n",
    "$\\frac{\\partial }{\\partial \\theta_j}J(\\theta) \\approx \\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon}$\n",
    "\n",
    "\n",
    "Implement gradApprox = (J(theta + EPSILON) - J(theta + EPSILON)) / (2*EPSILON)  as numerical estimate of the gradient\n",
    "\n",
    "\n",
    "\n",
    "#### Parameter Vector $\\theta$  \n",
    "$\\theta \\in \\mathbb{R}^{n}$ (E.g. $\\theta$ is unrolled version of $\\Theta^{(1)}, \\Theta^{(2)}, \\Theta^{(3)}$)\n",
    "$\\theta = \\begin{bmatrix}\\theta_0 & \\theta_1 & \\theta_2 & \\dots & \\theta_n \\end{bmatrix}$\n",
    "\n",
    "\n",
    "$\\frac{\\partial }{\\partial \\theta_1}J(\\theta) \\approx \n",
    "\\frac{J(\\theta_1 + \\epsilon, \\theta_2, \\dots, \\theta_n) - J(\\theta_1 - \\epsilon, \\theta_2, \\dots, \\theta_n)}{2\\epsilon}$    \n",
    "\n",
    "$\\frac{\\partial }{\\partial \\theta_2}J(\\theta) \\approx \n",
    "\\frac{J(\\theta_1, \\theta_2 + \\epsilon, \\dots, \\theta_n) - J(\\theta_1, \\theta_2 - \\epsilon, \\dots, \\theta_n)}{2\\epsilon}$\n",
    "\n",
    "$\\vdots$\n",
    "\n",
    "$\\frac{\\partial }{\\partial \\theta_n}J(\\theta) \\approx \n",
    "\\frac{J(\\theta_1, \\theta_2, \\dots, \\theta_n + \\epsilon) - J(\\theta_1, \\theta_2, \\dots, \\theta_n - \\epsilon)}{2\\epsilon}$\n",
    "\n",
    "\n",
    "\n",
    "E.g of implemetation  \n",
    "\n",
    "epsilon = 1e-4;\n",
    "for i = 1:n,\n",
    "  thetaPlus = theta;\n",
    "  thetaPlus(i) += epsilon;\n",
    "  thetaMinus = theta;\n",
    "  thetaMinus(i) -= epsilon;\n",
    "  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)\n",
    "end;\n",
    "\n",
    "Check that gradApprox $\\approx$ DVec   \n",
    "\n",
    "#### Implementation Note\n",
    "- Implement backprop to compute DVec (unrolled D1, D2, ...)\n",
    "- implement numerical gradiente check to compute gradApprox\n",
    "- Make sure they give similar values\n",
    "- Turn off gradient checking. Using backprop code for learning\n",
    "\n",
    "#### Important\n",
    "- Be sure to disable your gradient checking code before training your classifier if you run numerical gradient computation on every iteration of gradient descent the code will be very slow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initialization \n",
    "\n",
    "For gradient descent we need initial vales for $\\Theta$. \n",
    "If we initialize all to zero the derivative on each node of layer have the same value and corresponding the same value of theta.\n",
    "\n",
    "To avoid this problem initialize each $\\Theta_{ij}^{(l)}$ to random value in $[-\\epsilon, \\epsilon]$ Note this is unrelated with grad approx.\n",
    "\n",
    "e.g. \n",
    "\n",
    "If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.\n",
    "\n",
    "Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;         \n",
    "Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;      \n",
    "Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
