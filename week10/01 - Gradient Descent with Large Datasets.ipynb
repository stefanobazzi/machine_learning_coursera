{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning With Large Datasets\n",
    "\n",
    "Classify beetween confusable words: e.g to, two or then, than    \n",
    "\"It's not the best algorithm that wins. It's who has the most data.\"    \n",
    "\n",
    "#### Learning wiht large dataset\n",
    "\n",
    "E.g. m = 100,000,000 -> Computational problem to calculate the derivative of the gradient descent.   \n",
    "\n",
    "Sanity check : pick randomly example of m (e.g 1000) an train the algortihm for this subset. Plot the learning curves and increasing m if the algorithm works the variance will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Suppose to train a linear model with gradient descent, if M is very large e.g M = 300,000,000 the algorithm Batch gradient descent will be slow to train.\n",
    "\n",
    "#### Batch gradient descent\n",
    "\n",
    "$J_{train}(\\theta) = \\frac{1}{2m}\\sum_{i=0}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "Repeat {\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$      \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; (for every  $j = 0, \\dots, n$)\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "#### Stochastic gradient descent\n",
    "\n",
    "$cost(\\theta, (x^{(i)}, y^{(i)})) = \\frac{1}{2}(h_{\\theta}(x^{(i)}) - y^{(i)})^2$      \n",
    "\n",
    "$J_{train}(\\theta) = \\frac{1}{2m}\\displaystyle\\sum_{i=0}^{m}cost(\\theta, (x^{(i)}, y^{(i)}))$\n",
    "\n",
    "1. Randomly shuffle dataset (reorder training examples)\n",
    "2. Repeat {     \n",
    "    for $i = 0, \\dots, m$ {       \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_j := \\theta_j - (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$      \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; (for $j = 0, \\dots, n$)     \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;}     \n",
    "    }\n",
    "\n",
    "Every iteration will be faster beacuse we do not sum all the training examples, but every iteration will train with one examples at time. The stochastic gradient descent will converge following more randon direction respect a batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "Batch gradient descent: use all $m$ examples in each iteration      \n",
    "Stochastic gradient descent: use all 1 example in each iteration  \n",
    "Mini-Batch gradient descent: use all $b$ example in each iteration   \n",
    "\n",
    "$b$ = mini-batch size    \n",
    "Get $b$ examples e.g $b = 10$, $m = 1000$\n",
    "\n",
    "Repeat {     \n",
    "for $i = 1, 11, 21, 31, \\dots, 991$ {       \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_j := \\theta_j - \\alpha\\frac{1}{10}\\displaystyle\\sum_{k=i}^{i + 9}(h_\\theta(x^{(k)}) - y^{(k)})x_j^{(k)}$      \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; (for $j = 0, \\dots, n$)     \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;}     \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Convergence\n",
    "Batch gradient descen_\n",
    "- Plot $J_{train}(\\theta)$ as afunction of the number of iteration of gradient descent\n",
    "- $J_{train}(\\theta) = \\frac{1}{2m}\\sum_{i=0}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "Stochastic gradient descent:\n",
    "- $cost(\\theta, (x^{(i)}, y^{(i)})) = \\frac{1}{2}(h_{\\theta}(x^{(i)}) - y^{(i)})^2$ \n",
    "- During leanring compyte $cost(\\theta, (x^{(i)}, y^{(i)}))$ before updateing $\\theta$ using $(x^{(i)}, y^{(i)}))$\n",
    "- Every 1000 iterations(say), plot $cost(\\theta, (x^{(i)}, y^{(i)}))$ averaged over the last 1000 examples processed by algorithm.\n",
    "\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "\n",
    "$cost(\\theta, (x^{(i)}, y^{(i)})) = \\frac{1}{2}(h_{\\theta}(x^{(i)}) - y^{(i)})^2$      \n",
    "\n",
    "$J_{train}(\\theta) = \\frac{1}{2m}\\displaystyle\\sum_{i=0}^{m}cost(\\theta, (x^{(i)}, y^{(i)}))$\n",
    "\n",
    "1. Randomly shuffle dataset (reorder training examples)\n",
    "2. Repeat {     \n",
    "    for $i = 1, \\dots, m$ {       \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; $ \\theta_j := \\theta_j - \\alpha(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$      \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; (for $j = 0, \\dots, n$)     \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;}     \n",
    "    }     \n",
    "Learning rate is typically held constant. Caln slowly decrease $\\alpha$ over time if we want $\\theta$ to converge.    \n",
    "E.g $\\alpha = \\frac{const1}{iterationNum + const2}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
